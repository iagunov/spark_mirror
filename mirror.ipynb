{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac232452-e3bf-4188-ae3f-b394e4cd4b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 13:58:24 WARN Utils: Your hostname, NB99-186 resolves to a loopback address: 127.0.1.1; using 192.168.56.1 instead (on interface eth3)\n",
      "24/04/01 13:58:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/01 13:58:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating a Spark Session Block\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Callable\n",
    "from functools import wraps\n",
    "\n",
    "import findspark\n",
    "import mysql.connector\n",
    "import pyspark\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from mysql.connector import Error\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "findspark.init('/home/su/spark-3.5.1-bin-hadoop3')\n",
    "\n",
    "\n",
    "#Creating a Spark Session.\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName('pyspark_example')\n",
    " .config(\"spark.driver.extraClassPath\", \"/home/su/spark_mirror/mysql-connector-j-8.3.0.jar\")\n",
    " .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae87c039-d59d-461f-a349-4837c0a3724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating decorators block\n",
    "\n",
    "def execution_time(func: Callable) -> Callable:\n",
    "    \"\"\"\n",
    "    Decorator function to measure the execution time of a given function.\n",
    "    \n",
    "    :param func: Callable - The function to be measured for execution time.\n",
    "    :return Callable: The wrapped function with execution time logging.\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args: str, **kwargs: int) -> None:\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        logger.info(f'{args[2]} delta from table {args[1]} was executed for {execution_time}.',\n",
    "                    extra={'table_name': f'{args[1]}', 'delta_name': f'{args[2]}'})\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def error_log(func: Callable) -> Callable:\n",
    "    \"\"\"\n",
    "    Decorator function for handling errors and logging exceptions.\n",
    "    \n",
    "    :param func: Callable - The function to be wrapped for error handling.\n",
    "    :return Callable: The wrapped function with error logging capabilities.\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs) -> None:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error in function {func.__name__}: {e}',\n",
    "                         extra={'table_name': f'{args[1]}', 'delta_name': f'{args[2]}'})\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d781c52-108c-4493-91a0-da093a0398d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The setting up logging block\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "db_config = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'database': os.getenv(\"DB_NAME\"),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\")\n",
    "}\n",
    "\n",
    "\n",
    "class MirrorLogger(logging.Handler):\n",
    "    \"\"\"\n",
    "    Class for setting up logging to a MySQL database.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        logging.Handler.__init__(self)\n",
    "\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            level_value = record.levelname  # Getting the level of logging.\n",
    "            message_value = self.format(record)  # Getting a log message.\n",
    "            time_value = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(record.created)) # Getting the time of the action.\n",
    "            filename_value = record.filename # Getting the name of the file.\n",
    "            \n",
    "            connection = mysql.connector.connect(**db_config)\n",
    "            cursor = connection.cursor()\n",
    "            cursor.execute('INSERT INTO mirror_log (time, level, message, filename, table_name, delta_name) VALUES (%s, %s, %s, %s, %s, %s)',\n",
    "                           (time_value, level_value, message_value, filename_value, record.table_name, record.delta_name))\n",
    "            connection.commit()\n",
    "            if connection.is_connected():\n",
    "                cursor.close()\n",
    "                connection.close()\n",
    "        except Error as e:\n",
    "            print(f'Error while commecting to MySQL: {e}')\n",
    "\n",
    "\n",
    "# Setting up the level of logging.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    filemode='a',\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s | %(filename)s'\n",
    ")\n",
    "\n",
    "# Creating a log.\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Creating a log handler.\n",
    "db_handler = MirrorLogger()\n",
    "logger.addHandler(db_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1701ce0-57c9-4e2b-83c0-14bc8c360689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The block for creating auxiliary functions\n",
    "\n",
    "def create_logdb() -> None:\n",
    "    \"\"\"\n",
    "    Function to create a database and a table for logging in MySQL.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "                host=db_config.get('host'),\n",
    "                user=db_config.get('user'),\n",
    "                password=db_config.get('password')\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute('CREATE DATABASE IF NOT EXISTS logs')\n",
    "        cursor.execute('USE logs')\n",
    "    \n",
    "        cursor.execute('CREATE TABLE IF NOT EXISTS mirror_log ('\n",
    "                       'time DATETIME, '\n",
    "                       'level VARCHAR(50), '\n",
    "                       'message VARCHAR(300), '\n",
    "                       'filename VARCHAR(100), '\n",
    "                       'table_name VARCHAR(100), '\n",
    "                       'delta_name INT)'\n",
    "                       )\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    except Exception as e:\n",
    "        print('Unable to create a database for logs.')\n",
    "    \n",
    "\n",
    "\n",
    "@error_log\n",
    "def get_log() -> list[tuple[Any, ...]]:\n",
    "    \"\"\"\n",
    "    Function to retrieve log data from a MySQL database.\n",
    "    \n",
    "    :return list[tuple[Any, ...]]: A list of tuples representing log data.\n",
    "    \"\"\"\n",
    "    log = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/logs\") \\\n",
    "    .option(\"dbtable\", \"mirror_log\") \\\n",
    "    .option(\"user\", \"user1\") \\\n",
    "    .option(\"password\", \"12345678\") \\\n",
    "    .load()\n",
    "    \n",
    "    return log\n",
    "\n",
    "\n",
    "@error_log\n",
    "def get_mirror(db_path: str) -> list[tuple[Any, ...]]:\n",
    "    \"\"\"\n",
    "    Function to retrieve mirror data from a CSV file.\n",
    "\n",
    "    :param db_path: str - The path of the table for which delta is being created.\n",
    "    :return list[tuple[Any, ...]]: A list of tuples representing mirror data.\n",
    "    \"\"\"\n",
    "    mirror = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"delimiter\", \";\")\n",
    "        .load(f\"{db_path}/mirr_md_account_d\")\n",
    "    )\n",
    "\n",
    "    return mirror\n",
    "\n",
    "\n",
    "@execution_time\n",
    "@error_log\n",
    "def get_delta(db_path: str, table_name: str, delta_name: int, keys: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    The main function for creating a delta between mirror and delta data.\n",
    "\n",
    "    :param db_path: str - The path of the table for which delta is being created.\n",
    "    :param table_name: str - The name of the table for which delta is being created.\n",
    "    :param delta_name: int - The specific delta file to process.\n",
    "    :param key: list[str] - The list of keys used for joining mirror and delta data.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    logger.info(\n",
    "            f'Starting loading the {delta_name} delta.',\n",
    "            extra={'table_name': f'{table_name}', 'delta_name': f'{delta_name}'}\n",
    "        )\n",
    "    # Loading mirror and delta dataframes\n",
    "    df_mirror = get_mirror(db_path)\n",
    "    df_delta = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"delimiter\", \";\")\n",
    "        .load(f\"{db_path}/{table_name}/{delta_name}\")\n",
    "    )\n",
    "    \n",
    "    # Creating temporary views for mirror and delta dataframes\n",
    "    df_mirror.createOrReplaceTempView(\"mirror\")\n",
    "    df_delta.createOrReplaceTempView(\"delta\")\n",
    "\n",
    "    # Getting the list of fields for mirror and delta dataframes\n",
    "    mirror_fields = df_mirror.schema.names\n",
    "    delta_fields = df_delta.schema.names\n",
    "\n",
    "    # Creating field strings for coalescing data from mirror and delta\n",
    "    field_strings = [\n",
    "        f\"coalesce(d.{delta_field}, m.{mirror_field}) as {delta_field}\" for delta_field, mirror_field in zip(\n",
    "            delta_fields, mirror_fields\n",
    "        )\n",
    "    ]\n",
    "    selected_fields = ', '.join(field_strings)\n",
    "    \n",
    "    # Creating key conditions for joining mirror and delta data\n",
    "    key_conditions = ' AND '.join([f\"m.{key} = d.{key}\" for key in keys])\n",
    "    \n",
    "    # Executing SQL query to select fields and join mirror and delta data\n",
    "    mirror = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        {selected_fields}\n",
    "    FROM \n",
    "    mirror m FULL JOIN delta d ON {key_conditions}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Show the first 5 rows of the resulting dataframe\n",
    "    mirror.show(5)\n",
    "\n",
    "    # Writing a result.\n",
    "    path = f'{db_path}/mirr_md_account_d'\n",
    "    mirror.write.csv(path, mode='overwrite', header='True', sep=';')\n",
    "    logger.info(\n",
    "            f'Finishing loading the {delta_name} delta.',\n",
    "            extra={'table_name': f'{table_name}', 'delta_name': f'{delta_name}'}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0542393-4cd3-4af9-ae9f-aea489a04762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The main function block\n",
    "\n",
    "@error_log\n",
    "def merge_delta(db_path: str, table_name: str, keys: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    The function manages the merging of delta files for a specified table.\n",
    "\n",
    "    :param db_path: str - The path of the table for which delta is being created.\n",
    "    :param table_name: str - The name of the table to merge delta files for.\n",
    "    :param key: list[str] - The list of keys used for joining mirror and delta data.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Creating database for logs.\n",
    "    create_logdb()\n",
    "    # Retrieving log data and create a temporary view\n",
    "    log_check = get_log()\n",
    "    log_check.createOrReplaceTempView(\"log\")\n",
    "\n",
    "    #Setting up directory path and list files\n",
    "    directory = f\"{db_path}/{table_name}/\"\n",
    "    files = list(map(int, os.listdir(directory)))\n",
    "\n",
    "    # Determining delta files to process\n",
    "    dc_querry = \"select delta_name from log\"\n",
    "    delta_check = list((set(files) - set(\n",
    "        [row[0] for row in (spark.sql(dc_querry).collect())]))\n",
    "                      )\n",
    "    i = delta_check[0] if delta_check else 1000\n",
    "\n",
    "    # Checking if log table is empty and create mirror if needed\n",
    "    if (spark.sql(\"select count(*) from log\").collect()[0][0]) == 0:\n",
    "        logger.info(\n",
    "            'Srarting loading the mirror.',\n",
    "            extra={'table_name': f'{table_name}', 'delta_name': f'{i}'}\n",
    "        )\n",
    "        df = (\n",
    "            spark.read.format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"delimiter\", \";\")\n",
    "            .load(f\"{db_path}/{table_name}/1000\")\n",
    "        )\n",
    "\n",
    "        # Writing mirror data and log the action\n",
    "        path = f'{db_path}/mirr_md_account_d'\n",
    "        df.write.csv(path, mode='overwrite', header='True', sep=';')\n",
    "        logger.info(\n",
    "            'Finishing loading the mirror.',\n",
    "            extra={'table_name': f'{table_name}', 'delta_name': f'{i}'}\n",
    "        )\n",
    "        logger.info(\n",
    "            'Mirror was created.',\n",
    "            extra={'table_name': f'{table_name}', 'delta_name': f'{i}'}\n",
    "        )\n",
    "        df.show(5)\n",
    "        i += 1\n",
    "\n",
    "    # Processing delta files one by one\n",
    "    dn_querry = \"select delta_name from log\"\n",
    "    while i in files and i not in [row[0] for row in (spark.sql(dn_querry).collect())]:\n",
    "        get_delta(db_path, table_name, i, keys)\n",
    "        logger.info(\n",
    "            f'{i} delta was merged.',\n",
    "            extra={'table_name': f'{table_name}', 'delta_name': f'{i}'}\n",
    "        )\n",
    "        i += 1\n",
    "\n",
    "merge_delta('/home/su/spark_mirror', 'data_deltas', ['account_rk'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
